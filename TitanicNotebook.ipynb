{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##show logs about the dataset, such as nan values\n",
    "show_data_info = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove useless data columns like passengerid, name, cabin (just for semplicty) and ticket number\n",
    "def remove_extra_features(data):\n",
    "    return data.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis=1)\n",
    "\n",
    "def map_data(data):\n",
    "    ##map every string in the dataset, we can only have numbers\n",
    "    sex_class_mapping = {label: idx for idx, label in enumerate(np.unique(data[\"Sex\"]))}\n",
    "    ##the final astype ensures the object is treated like a string\n",
    "    embarked_class_mapping = {label: idx for idx, label in enumerate(np.unique(data[\"Embarked\"].astype(str)))}\n",
    "    data[\"Sex\"] = data[\"Sex\"].map(sex_class_mapping)\n",
    "    data[\"Embarked\"] = data[\"Embarked\"].map(embarked_class_mapping)\n",
    "    data['Fare'] = data['Fare'].astype(int)\n",
    "    return data\n",
    "\n",
    "def manage_nan_values(data):\n",
    "    ##manage NaN values\n",
    "    imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imr = imr.fit(data)\n",
    "    return pd.DataFrame(imr.transform(data.values))\n",
    "\n",
    "def plot_data(x, y, title):\n",
    "    #perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(x)  \n",
    "    x_pca = pca.transform(x)\n",
    "    #plot PCA\n",
    "    colors = [\"r\", \"b\", \"g\"]\n",
    "    markers = [\"s\", \"x\", \"o\"]\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(x_pca[y==l, 0], x_pca[y==l, 1], c=c, marker=m, label=l)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "331       1    male  45.5      0      0  28.5000        S\n",
      "733       2    male  23.0      0      0  13.0000        S\n",
      "382       3    male  32.0      0      0   7.9250        S\n",
      "704       3    male  26.0      1      0   7.8542        S\n",
      "813       3  female   6.0      4      2  31.2750        S\n",
      "  Pclass  Sex   Age SibSp Parch  Fare  Embarked\n",
      "0      3    0  92.0     0     0     1         1\n",
      "1      3    1  22.0     0     0    12         0\n",
      "2      3    0  47.0     1     0     4         0\n",
      "3      2    0  21.0     1     0    17         0\n",
      "4      2    1  29.0     1     0    17         0\n",
      "5      3    0  22.0     0     0     7         0\n",
      "     0    1     2    3    4     5    6\n",
      "0  3.0  0.0  92.0  0.0  0.0   1.0  1.0\n",
      "1  3.0  1.0  22.0  0.0  0.0  12.0  0.0\n",
      "2  3.0  0.0  47.0  1.0  0.0   4.0  0.0\n",
      "3  2.0  0.0  21.0  1.0  0.0  17.0  0.0\n",
      "4  2.0  1.0  29.0  1.0  0.0  17.0  0.0\n",
      "5  3.0  0.0  22.0  0.0  0.0   7.0  0.0\n",
      "          0         1         2         3         4         5         6\n",
      "0  0.813034 -1.380624  4.813270 -0.470722 -0.479342 -0.600018 -0.729503\n",
      "1  0.813034  0.724310 -0.577493 -0.470722 -0.479342 -0.388260 -2.024371\n",
      "2  0.813034 -1.380624  1.347780  0.379923 -0.479342 -0.542266 -2.024371\n",
      "3 -0.400551 -1.380624 -0.654504  0.379923 -0.479342 -0.292006 -2.024371\n",
      "4 -0.400551  0.724310 -0.038417  0.379923 -0.479342 -0.292006 -2.024371\n",
      "5  0.813034 -1.380624 -0.577493 -0.470722 -0.479342 -0.484514 -2.024371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/animalecs/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/animalecs/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/animalecs/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/train.csv\") \n",
    "\n",
    "y = data.Survived\n",
    "X = data.drop(\"Survived\", axis=1)\n",
    "X = remove_extra_features(X)\n",
    "labels = X.columns[1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.head())\n",
    "\n",
    "extra_dataframe = pd.DataFrame(columns=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "extra_dataframe = extra_dataframe.append(pd.Series([3, \"female\", 92.0, 0, 0, 1.0, \"S\"], index=extra_dataframe.columns ), ignore_index=True)\n",
    "extra_dataframe = extra_dataframe.append(pd.Series([3, \"male\", 22.0, 0, 0, 12.0, \"C\"], index=extra_dataframe.columns ), ignore_index=True)\n",
    "extra_dataframe = extra_dataframe.append(pd.Series([3, \"female\", 47.0, 1, 0, 4.0, \"C\"], index=extra_dataframe.columns ), ignore_index=True)\n",
    "extra_dataframe = extra_dataframe.append(pd.Series([2, \"female\", 21.0, 1, 0, 17.0, \"C\"], index=extra_dataframe.columns ), ignore_index=True)\n",
    "extra_dataframe = extra_dataframe.append(pd.Series([2, \"male\", 29.0, 1, 0, 17.0, \"C\"], index=extra_dataframe.columns ), ignore_index=True)\n",
    "extra_dataframe = extra_dataframe.append(pd.Series([3, \"female\", 22.0, 0, 0, 7.0, \"C\"], index=extra_dataframe.columns ), ignore_index=True)\n",
    "\n",
    "\n",
    "if show_data_info:\n",
    "    ##show percentage of missing data that can mislead our prediction\n",
    "    print(\"Percentage of missing data in the train_set\\r\\n \", X_train.isnull().sum() * 100 / X_train.shape[0])\n",
    "    print(\"Percentage of missing data in the test_set\\r\\n \", X_test.isnull().sum() * 100 / X_test.shape[0])\n",
    "\n",
    "##map string values in the dataset\n",
    "X_train = map_data(X_train)\n",
    "X_test = map_data(X_test)\n",
    "extra_dataframe = map_data(extra_dataframe)\n",
    "\n",
    "print(extra_dataframe)\n",
    "##manage NaN values\n",
    "X_train = manage_nan_values(X_train)\n",
    "X_test = manage_nan_values(X_test)\n",
    "extra_dataframe = manage_nan_values(extra_dataframe)\n",
    "\n",
    "\n",
    "print(extra_dataframe)\n",
    "\n",
    "##scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "extra_dataframe = pd.DataFrame(scaler.transform(extra_dataframe))\n",
    "\n",
    "\n",
    "print(extra_dataframe)\n",
    "\n",
    "#plot_data(X_train, y_train, \"train data\")\n",
    "#plot_data(X_test, y_test, \"test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest accuracy 0.826816\n",
      "0) Dead\n",
      "1) Dead\n",
      "2) Dead\n",
      "3) Survived\n",
      "4) Dead\n",
      "5) Survived\n"
     ]
    }
   ],
   "source": [
    "##tuning parameters\n",
    "# this requires A LOT of CPU power\n",
    "## Best params  {'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 16, 'n_estimators': 700}\n",
    "#param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n",
    "#rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "#clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n",
    "#clf.fit(X_train, y_train)\n",
    "#print(\"Best params \",clf.best_params_)\n",
    "\n",
    "##n_jobs = -1 to use all processors available\n",
    "forest = RandomForestClassifier(n_estimators=1600, random_state=0, n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "##checking most important features in our dataset\n",
    "if show_data_info:\n",
    "    importances_dict = dict(zip(labels, np.around(forest.feature_importances_, decimals=2))) \n",
    "    print(sorted(importances_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "\n",
    "forest_prediction = forest.predict(X_test)\n",
    "\n",
    "print(\"Forest accuracy %2f\" % accuracy_score(y_test, forest_prediction))\n",
    "\n",
    "people_prediction = forest.predict(extra_dataframe)\n",
    "for result in enumerate(people_prediction):\n",
    "    print(\"%d) %s\" % (result[0] , \"Dead\" if result[1] == 0 else \"Survived\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
